{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "\n",
    "Construct a multi layer perceptron with 3 layers for the same dataset, train it, and see\n",
    "whether the result improves or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import necessary libraries\n",
    "import os \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilitty Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sigmoid activation function \n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ReLu function \n",
    "def relu(x):\n",
    "    # Return x of x > 0 or 0 if x <= 0\n",
    "    return x * (x > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sigmoid derivative function\n",
    "def der_sigmoid(x): \n",
    "    sigmoid(x)*(1-sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a cost function \n",
    "def cost_function1(pred, Y):\n",
    "    \n",
    "    # Number of samples is m\n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    # Compute the cross-entropy cost\n",
    "    logprobs = Y * np.log(pred) + (1-Y) * np.log(1-pred)\n",
    "    cost = -1/m * np.sum(logprobs)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(pred, Y):\n",
    "    \n",
    "    cost = np.mean(np.power(Y.T - pred.T, 2));\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((190, 2049), (20, 2049))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('../Datasets/final_data.csv', index_col=0)\n",
    "test_data = pd.read_csv('../Datasets/test_data.csv', index_col=0)\n",
    "\n",
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign classes as 0s and 1s \n",
    "train_data['class'] = train_data['class'].map({ \"cat\":0, \"dog\":1 })\n",
    "test_data['class'] = test_data['class'].map({ \"cat\":0, \"dog\":1 })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the MLP\n",
    "\n",
    "1. Initialize the weights and biases. Weights are randomly initialized while biases are given 0s. Save these "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    \n",
    "    # Initialize the instance variables \n",
    "    def __init__(self, input_size, hidden_units, output_size):\n",
    "        \n",
    "        # Initialize the weights and the biases for the layers \n",
    "        W1 = np.random.randn(hidden_units, input_size)\n",
    "        b1 = np.zeros(shape=(hidden_units, 1))\n",
    "        \n",
    "        W2 = np.random.randn(output_size, hidden_units)\n",
    "        b2 = np.zeros(shape=(output_size, 1))\n",
    "        \n",
    "        # Show details of the matrices \n",
    "        print(f\"Dimensions of the hidden layer weights: {W1.shape} and dimensions of the bias vector: {b1.shape}\")\n",
    "        print(f\"\\nDimensions of the output layer weights: {W2.shape} and dimensions of the bias vector: {b2.shape}\")\n",
    "        \n",
    "        # We can save them in a parameters dictionary \n",
    "        self.params = {\"W1\": W1,\n",
    "                       \"b1\": b1,\n",
    "                       \"W2\": W2,\n",
    "                       \"b2\": b2}\n",
    "    \n",
    "    # Forward propagation function \n",
    "    def forward_prop(self, X):\n",
    "        \n",
    "        # Perform forward prop for both the layers\n",
    "        \n",
    "        # Extract the layer parameters\n",
    "        W1 = self.params['W1']\n",
    "        b1 = self.params['b1']\n",
    "        W2 = self.params['W2']\n",
    "        b2 = self.params['b2']\n",
    "        \n",
    "        # Forward prop for layer 1\n",
    "        z1 = np.dot(W1, X) + b1\n",
    "        a1 = relu(z1)\n",
    "        print(a1)\n",
    "        print(\"Shape of the feature matrix after the first pass\", a1.shape)\n",
    "        \n",
    "        # Forward prop for layer 2\n",
    "        z2 = np.dot(W2, a1) + b2\n",
    "        a2 = sigmoid(z2)\n",
    "        print(a2)\n",
    "        print(\"Shape of the feature matrix after the second pass\", a2.shape)\n",
    "        \n",
    "        # Store the propagation outputs\n",
    "        self.forward_outputs = {\"z1\": z1,\n",
    "                           \"a1\": a1,\n",
    "                           \"z2\": z2,\n",
    "                           \"a2\": a2}\n",
    "    \n",
    "    # Define a backward propagation function \n",
    "    def backward_prop(self, X, Y):\n",
    "        \n",
    "        # Total units \n",
    "        m = X.shape[1]\n",
    "        \n",
    "        # Collect the relevant parameters \n",
    "        W1 = self.params['W1']\n",
    "        W2 = self.params['W2']\n",
    "        a1 = self.forward_outputs['a1']\n",
    "        a2 = self.forward_outputs['a2']\n",
    "        \n",
    "        # Start backprop from the output unit \n",
    "        dZ2 = a2 - Y\n",
    "        dW2 = 1/m * dZ2 @ a1.T\n",
    "        db2 = 1/m * np.sum(dZ2, axis=1, keepdims=True)\n",
    "        dZ1 = W2.T @ dZ2 * (1 - a1**2) \n",
    "        dW1 = 1/m * dZ1 @ X.T\n",
    "        db1 = 1/m * np.sum(dZ1, axis=1, keepdims=True)\n",
    "        \n",
    "        # collect gradients\n",
    "        self.grads = {\"dW1\": dW1,\n",
    "                      \"db1\": db1,\n",
    "                      \"dW2\": dW2,\n",
    "                      \"db2\": db2}\n",
    "    \n",
    "    # Function to update the parameters \n",
    "    def update_params(self, learning_rate=1):\n",
    "        \n",
    "        # Get grads and params \n",
    "        W1 = self.params['W1']\n",
    "        b1 = self.params['b1']\n",
    "        W2 = self.params['W2']\n",
    "        b2 = self.params['b2']\n",
    "        \n",
    "        dW1 = self.grads['dW1']\n",
    "        db1 = self.grads['db1']\n",
    "        dW2 = self.grads['dW2']\n",
    "        db2 = self.grads['db2']\n",
    "        \n",
    "        # Update step \n",
    "        W1 = W1 - learning_rate * dW1\n",
    "        b1 = b1 - learning_rate * db1\n",
    "        W2 = W2 - learning_rate * dW2\n",
    "        b2 = b2 - learning_rate * db2\n",
    "        \n",
    "        # Reset parameter values \n",
    "        self.params = {\"W1\": W1,\n",
    "                       \"b1\": b1,\n",
    "                       \"W2\": W2,\n",
    "                       \"b2\": b2}\n",
    "\n",
    "    \n",
    "    # Create a function to train the neural network \n",
    "    def fit(self, X, y, epochs=5, learning_rate=0.1):\n",
    "        \n",
    "        print(\"\\nTraining for \", epochs, \" epochs\")\n",
    "        \n",
    "        # Iterate through all epochs\n",
    "        for e in range(epochs):\n",
    "            print(\"Training Epoch: \", e+1)\n",
    "            \n",
    "            # Perform the forward prop step \n",
    "            self.forward_prop(X)\n",
    "            \n",
    "            # Compute the cost \n",
    "            preds = self.forward_outputs['a2']\n",
    "            cost = cost_function(pred=preds, Y=y)\n",
    "            print(\"The cost after this step:\", cost)\n",
    "            \n",
    "            # Backprop step \n",
    "            self.backward_prop(X, y)\n",
    "            \n",
    "            # Update the parameters\n",
    "            self.update_params(learning_rate=1)\n",
    "       \n",
    "    # Prediction function \n",
    "    def predict(self, X):\n",
    "        \n",
    "        # Perform a forward prop step\n",
    "        self.forward_prop(X)\n",
    "        a2 = self.forward_outputs['a2']\n",
    "        \n",
    "        # 1 if a2 > 0.5 else 0\n",
    "        preds = (a2 > 0.5) * 1\n",
    "        \n",
    "        return preds\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048, 190) (2048, 20) (1, 190) (1, 190)\n",
      "\n",
      "Number of training samples 190\n"
     ]
    }
   ],
   "source": [
    "# Get the training and testing data \n",
    "X_train = train_data.drop(['class'], axis=1).values.T\n",
    "X_test = test_data.drop(['class'], axis=1).values.T\n",
    "\n",
    "y_train = train_data[['class']].to_numpy().T\n",
    "y_test = test_data[['class']].to_numpy().T\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_train.shape)\n",
    "print(\"\\nNumber of training samples\", X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the initial params\n",
    "input_size = X_train.shape[0]\n",
    "hidden_units = 128\n",
    "output_size = y_train.shape[0]\n",
    "\n",
    "# Hyperparameters\n",
    "epochs=5\n",
    "learning_rate=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of the hidden layer weights: (128, 2048) and dimensions of the bias vector: (128, 1)\n",
      "\n",
      "Dimensions of the output layer weights: (1, 128) and dimensions of the bias vector: (1, 1)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the MLP \n",
    "mlp = MLP(input_size, hidden_units, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training for  5  epochs\n",
      "Training Epoch:  1\n",
      "[[   -0.            -0.            -0.         ... 45723.79184968\n",
      "     -0.         10568.13838534]\n",
      " [   -0.         31576.09552567    -0.         ... 12853.26702586\n",
      "   3156.24074705 34725.39641673]\n",
      " [ 5415.25054025 18460.9627581     -0.         ...    -0.\n",
      "     -0.            -0.        ]\n",
      " ...\n",
      " [26299.44833916    -0.          9818.91906582 ...  9402.36689914\n",
      "   6445.61613976    -0.        ]\n",
      " [   -0.         10580.65453939    -0.         ...    -0.\n",
      "   5589.02548293  9719.93809793]\n",
      " [  884.95850326  4284.52023817    -0.         ...    -0.\n",
      "     -0.          4411.36135293]]\n",
      "Shape of the feature matrix after the first pass (128, 190)\n",
      "[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1.]]\n",
      "Shape of the feature matrix after the second pass (1, 190)\n",
      "The cost after this step: 0.5\n",
      "Training Epoch:  2\n",
      "[[-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 ... -0.00000000e+00\n",
      "  -0.00000000e+00 -0.00000000e+00]\n",
      " [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 ... -0.00000000e+00\n",
      "  -0.00000000e+00 -0.00000000e+00]\n",
      " [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 ... -0.00000000e+00\n",
      "  -0.00000000e+00 -0.00000000e+00]\n",
      " ...\n",
      " [ 2.12593864e+15  2.12942385e+15  3.05335186e+14 ...  4.93762994e+15\n",
      "   6.89118845e+14  2.97047491e+15]\n",
      " [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 ... -0.00000000e+00\n",
      "  -0.00000000e+00 -0.00000000e+00]\n",
      " [ 9.41398849e+16  8.08248334e+16  1.51541734e+16 ...  2.01060793e+17\n",
      "   2.33004242e+16  1.34083329e+17]]\n",
      "Shape of the feature matrix after the first pass (128, 190)\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "Shape of the feature matrix after the second pass (1, 190)\n",
      "The cost after this step: 0.5\n",
      "Training Epoch:  3\n",
      "[[-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 ... -0.00000000e+00\n",
      "  -0.00000000e+00 -0.00000000e+00]\n",
      " [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 ... -0.00000000e+00\n",
      "  -0.00000000e+00 -0.00000000e+00]\n",
      " [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 ... -0.00000000e+00\n",
      "  -0.00000000e+00 -0.00000000e+00]\n",
      " ...\n",
      " [ 1.83322438e+43  1.62547223e+43  2.83717573e+42 ...  3.98244867e+43\n",
      "   4.63751308e+42  2.51629153e+43]\n",
      " [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 ... -0.00000000e+00\n",
      "  -0.00000000e+00 -0.00000000e+00]\n",
      " [ 5.99646145e+46  5.12859319e+46  9.60841938e+45 ...  1.26510008e+47\n",
      "   1.44081364e+46  8.41155460e+46]]\n",
      "Shape of the feature matrix after the first pass (128, 190)\n",
      "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "Shape of the feature matrix after the second pass (1, 190)\n",
      "The cost after this step: 0.5\n",
      "Training Epoch:  4\n",
      "[[-0.00000000e+000 -0.00000000e+000 -0.00000000e+000 ... -0.00000000e+000\n",
      "  -0.00000000e+000 -0.00000000e+000]\n",
      " [-0.00000000e+000 -0.00000000e+000 -0.00000000e+000 ... -0.00000000e+000\n",
      "  -0.00000000e+000 -0.00000000e+000]\n",
      " [-0.00000000e+000 -0.00000000e+000 -0.00000000e+000 ... -0.00000000e+000\n",
      "  -0.00000000e+000 -0.00000000e+000]\n",
      " ...\n",
      " [ 1.50984035e+111  1.30550283e+111  2.38955555e+110 ...  3.21167903e+111\n",
      "   3.68857179e+110  2.09428567e+111]\n",
      " [-0.00000000e+000 -0.00000000e+000 -0.00000000e+000 ... -0.00000000e+000\n",
      "  -0.00000000e+000 -0.00000000e+000]\n",
      " [ 5.87919365e+119  5.00725220e+119  9.43025869e+118 ...  1.23380638e+120\n",
      "   1.40575872e+119  8.21460154e+119]]\n",
      "Shape of the feature matrix after the first pass (128, 190)\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "Shape of the feature matrix after the second pass (1, 190)\n",
      "The cost after this step: 0.5\n",
      "Training Epoch:  5\n",
      "[[-0.00000000e+000 -0.00000000e+000 -0.00000000e+000 ... -0.00000000e+000\n",
      "  -0.00000000e+000 -0.00000000e+000]\n",
      " [-0.00000000e+000 -0.00000000e+000 -0.00000000e+000 ... -0.00000000e+000\n",
      "  -0.00000000e+000 -0.00000000e+000]\n",
      " [-0.00000000e+000 -0.00000000e+000 -0.00000000e+000 ... -0.00000000e+000\n",
      "  -0.00000000e+000 -0.00000000e+000]\n",
      " ...\n",
      " [ 7.68329021e+274  6.57983675e+274  1.22600340e+274 ...  1.62003269e+275\n",
      "   1.85240515e+274  1.06950063e+275]\n",
      " [-0.00000000e+000 -0.00000000e+000 -0.00000000e+000 ... -0.00000000e+000\n",
      "  -0.00000000e+000 -0.00000000e+000]\n",
      " [ 3.60963758e+295  3.07220571e+295  5.79044388e+294 ...  7.56893933e+295\n",
      "   8.62566720e+294  5.03868734e+295]]\n",
      "Shape of the feature matrix after the first pass (128, 190)\n",
      "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "Shape of the feature matrix after the second pass (1, 190)\n",
      "The cost after this step: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:68: RuntimeWarning: overflow encountered in square\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:68: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    }
   ],
   "source": [
    "# Train on the data\n",
    "mlp.fit(X=X_train, y=y_train, epochs=epochs, learning_rate=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0. -0. -0. ... -0. -0. -0.]\n",
      " [-0. -0. -0. ... -0. -0. -0.]\n",
      " [-0. -0. -0. ... -0. -0. -0.]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [-0. -0. -0. ... -0. -0. -0.]\n",
      " [nan nan nan ... nan nan nan]]\n",
      "Shape of the feature matrix after the first pass (128, 20)\n",
      "[[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan]]\n",
      "Shape of the feature matrix after the second pass (1, 20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in greater\n",
      "  after removing the cwd from sys.path.\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:136: RuntimeWarning: invalid value encountered in greater\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
